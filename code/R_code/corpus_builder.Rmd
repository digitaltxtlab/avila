---
title: "Avila Corpus Builder"
author: "Isa Lykke Hansen - i@lykkeh.dk"
date: "6/7/2017"
output: html_document
---

This markdown creates a clean corpus containing all the Avila texts and the metadata partaining to these  
The script;  
1. imports the files, 
2. adds the metadata
3. cleans the corpus and returns and saves a clean corpus

In the cleaning process, all footnotes are removed from the corpus - if you wish to include these (or filter out others), you can change the tm_filter line in the clean_corpus function  
Lastly, the corpus is saved as both a .txt and an .rda file, which can also be found in the "data" folder

In the final section I have included some useful code for inspection of the Corpus.  
The Document Term Matrix (dtm) used in the structural analysis is created here and can also be found in the "data" folder as an .rda file

```{r setup, include=FALSE} 
#Setup

knitr::opts_chunk$set(echo = TRUE)

library(pacman)
p_load(tm, #for textmining
       SnowballC, #for stemming
       tools, #for removing filepaths
       gtools, #for sorting files
       textreg, #for saving corpus to Rdata
       tidyverse) #for piping

wd <- "~/your/filepath/here/avila/" #NB change this working directory so it points to your local avila folder!
setwd(wd) 

```

###Import data  
Imports all the datafiles from the data_plain folder into a "Volatile Corpus"  
```{r}
files <- list.files(path = "data/plain_data") #make a list of all the files
files <- mixedsort(files) #sort the list numerically

#make a Source object to tell VCorpus which order to import the files in:
us <- paste(wd, "data/plain_data/", sep='') %>%
  paste(files, sep='') %>%
  URISource(encoding = "UTF-8", mode = "text")

books_cor  <- VCorpus(us, readerControl = list(language = "lat")) #import all texts in the plain data folder to a corpus

```

###Add metadata  
Adds all the metadata from the metadata csv to the Versatile Corpus created above  
Also adds a "filename" variable, extracted from the name of the file
  
Note: In the csv file the following acromyms are used in the "format" column:  
tes= spiritual testimonies  
sol=soliloquies  
minor= minor works  

```{r}

meta <- read.csv(paste(wd, "/data/meta_data/metadata.csv", sep=''), sep = ";") #import the metadata

filenames <- file_path_sans_ext(names(books_cor)) #extract the filenames without the ending

# add all the external metadata at document level

for (i in 1:length(books_cor)){
  books_cor[[i]]$meta$document_no <- as.character(meta$document_no[[i]])
  books_cor[[i]]$meta$name <- as.character(meta$name[[i]])
  books_cor[[i]]$meta$format <- as.character(meta$format[[i]])
  books_cor[[i]]$meta$year <- as.character(meta$year[[i]])
  books_cor[[i]]$meta$month <- as.character(meta$month[[i]])
  books_cor[[i]]$meta$day <- as.character(meta$day[[i]])
  books_cor[[i]]$meta$sender_city <- as.character(meta$sender_city[[i]])
  books_cor[[i]]$meta$reciever <- as.character(meta$reciever[[i]])
  books_cor[[i]]$meta$reciever_city <- as.character(meta$reciever_city[[i]])
  books_cor[[i]]$meta$incomplete <- as.character(meta$incomplete[[i]])
  books_cor[[i]]$meta$no_chapters <- as.character(meta$no_chapters[[i]])
  books_cor[[i]]$meta$book <- as.character(meta$book[[i]])
  books_cor[[i]]$meta$notes <- as.character(meta$notes[[i]])
  books_cor[[i]]$meta$translator <- as.character(meta$translator[[i]])
  books_cor[[i]]$meta$filename <- as.character(filenames[[i]])
}

```

###Clean data  
Finally, we clean the corpus, by mapping a series of tm functions to all the texts
```{r}

stop_words = as.character(read.table(paste(wd, "/data/stopword_us.txt", sep=''))[,1]) #import our own, more elaborate list of words to be removed from the text

# clean corpus

clean_corpus_function <- function(corpus){
  tm_map(corpus, content_transformer(tolower)) %>% #transforms all letters to lowercase
  tm_map(removePunctuation) %>% #removes punctuationmarks
  tm_map(removeNumbers) %>% #removes numbers
  tm_map(removeWords, stop_words) %>% #removes all words in stopword_us.txt. Alternatively use: stopwords("english")
  tm_map(stemDocument) %>% #transforms all words to their word stem
  tm_map(stripWhitespace) %>% #removes excess whitespace
  tm_filter(FUN = function(x) meta(x)[["format"]] !="footnote") %>% #remove all the footnotes from the corpus
  return() 
}

clean_corpus <- clean_corpus_function(books_cor) #returns the clean corpus
#create corpus without missing years:
remove_na_year <- !is.na(meta(clean_corpus, "year"))
clean_corpus_year <- clean_corpus[remove_na_year]


save.corpus.to.files(clean_corpus, filename = "data/clean_corpus") #saves the cleaned corpus to a .txt and .RData file

```

Additional useful code for inspecting the corpus  
Source: https://eight2late.wordpress.com/2015/05/27/a-gentle-introduction-to-text-mining-using-r/

```{r}

#inspects the text in the specified document
writeLines(as.character(clean_corpus[[30]]))

#make a matrix with documents in rows and unique terms in columns
dtm <- DocumentTermMatrix(clean_corpus)
dtm_year <- DocumentTermMatrix(clean_corpus_year)

#inspect a specific part of the matrix
inspect(dtm[1:9,1700:1716])

#find the frequency of different words
freq <- colSums(as.matrix(dtm))
length(freq)
#order the list, according to frequency
ord=order(freq, decreasing = TRUE)

#find the most and least frequent words
freq[head(ord)]
freq[tail(ord)]
#we can also look at all words that appear more than a specified number of times
findFreqTerms(dtm, lowfreq = 100)

save(dtm, file = "data/avila_dtm")
save(dtm_year, file = "data/avila_dtm_year")

```

