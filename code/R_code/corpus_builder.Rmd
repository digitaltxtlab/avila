---
title: "Avila Corpus Builder"
author: "Isa Lykke Hansen - i@lykkeh.dk"
date: "6/7/2017"
output: html_document
---

This markdown is ment for creating a clean corpus containing all the avila texts and the metadata partaining to these

```{r setup, include=FALSE} 
#Setup

knitr::opts_chunk$set(echo = TRUE)

library(pacman)
p_load(tm, #for textmining
       SnowballC, #for stemming
       tools, #for removing filepath
       gtools) #for sorting files

wd <- "~/Dropbox/IMC/Avila/" #NB change this working directory so it points to your local Avila folder
setwd(wd) 

```

###Import data  
Imports all the datafiles from the data_plain folder into a "Volatile Corpus"  
```{r}

files <- list.files(path="data/plain_data")
files <- mixedsort(files)

us <- paste(wd, "data/plain_data/",sep='') %>%
  paste(files, sep='') %>%
  URISource(encoding = "UTF-8",mode = "text")

books.cor  <- VCorpus(us, readerControl = list(language = "lat")) #import all texts in the plain data folder to a corpus

```

###Add metadata
Adds all the metadata from the metadata csv to the Versatile Corpus created above  
Also adds a "filename" variable, extracted from the name of the file
  
Note: In the csv file the following acromyms are used in the "format" column:  
tes= spiritual testimonies  
sol=soliloquies  
minor= minor works  

```{r}

meta <- read.csv(paste(wd,"/data/meta_data/metadata.csv", sep=''),sep = ";") #import the metadata

filenames <- file_path_sans_ext(names(books.cor)) #extract the filenames without the ending

# add all the external metadata at document level

for (i in 1:length(books.cor)){
  books.cor[[i]]$meta$document_no <- as.character(meta$document_no[[i]])
  books.cor[[i]]$meta$name <- as.character(meta$name[[i]])
  books.cor[[i]]$meta$format <- as.character(meta$format[[i]])
  books.cor[[i]]$meta$year <- as.character(meta$year[[i]])
  books.cor[[i]]$meta$month <- as.character(meta$month[[i]])
  books.cor[[i]]$meta$day <- as.character(meta$day[[i]])
  books.cor[[i]]$meta$sender_city <- as.character(meta$sendercity[[i]])
  books.cor[[i]]$meta$reciever <- as.character(meta$narecieverme[[i]])
  books.cor[[i]]$meta$reciever_city <- as.character(meta$reciever_city[[i]])
  books.cor[[i]]$meta$incomplete <- as.character(meta$incomplete[[i]])
  books.cor[[i]]$meta$no_chapters <- as.character(meta$no_chapters[[i]])
  books.cor[[i]]$meta$book <- as.character(meta$book[[i]])
  books.cor[[i]]$meta$notes <- as.character(meta$notes[[i]])
  books.cor[[i]]$meta$translator <- as.character(meta$translator[[i]])
  books.cor[[i]]$meta$filename <- as.character(filenames[[i]])
}

```

###Clean data  
Finally, we clean the corpus, by mapping a series of tm functions to all the texts
```{r}

stop.words = as.character(read.table(paste(wd,"/data/stopword_us.txt",sep=''))[,1]) #import our own, more elaborate list of words to be removed from the text

# clean corpus
clean_corpus <- function(corpus){
  #main <- tm_map(books.cor, PlainTextDocument) #is this important? - because it removes the metadata
  main <- tm_map(books.cor, content_transformer(tolower)) #transforms all letters to lowercase
  main <- tm_map(main, removePunctuation) #removes punctuationmarks
  main <- tm_map(main, removeNumbers) #removes numbers
  main <- tm_map(main, removeWords, stop.words) #alternatively, use: stopwords("english")
  main <- tm_map(main, stemDocument) #transforms all words to their word stem
  main <- tm_map(main, stripWhitespace) #removes excess whitespace
  return(main)
}

clean.corpus <- clean_corpus(books.cor) #returns the cleaned corpus

```
